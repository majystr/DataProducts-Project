Data Products: K-means Clustering of IRIS data
========================================================
author: Kenneth D. Graves
date: `r date()`
font-import: http://fonts.googleapis.com/css?family=Risque
font-family: 'Risque'

A R Presentation on a **shiny** application prepared for Coursera's Data Products course.  The application allows the selection of number of K clusters on the IRIS data-set to demonstrate the parameter's effect on K-means grouping in R.

K-Means Clustering
========================================================
left: 60%

K-means clustering aims to partition *n* observations into *k* clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. Given an initial set of centers, the k-means algorithm alternates two steps:

- for each center identify the subset of training points that are closer to them than any other centroid.
- the means each gouping are computed, and this mean vector becomes the new centroid for that cluster.

The steps are iterated until convergance.

***

![alt text](kmeans.png)

The IRIS Dataset
========================================================
left: 70%
To demonstrate the inital *k* parameter effect on R's k-means alogorthm, the application uses Edgar Anderson's IRIS dataset.  The data gives the measurements in centimeter of sepal and petal width and length.  The species are *Iris setosa, veriscolor*, and *virginica*.

Here are the actual means of each of the three species:

```{r, results='asis', echo=FALSE}
library("xtable")
mu <- aggregate(iris[,-5], by=list("Species" = iris$Species), mean)
print(xtable(mu),type='html',comment=FALSE)
```

***

![alt text](Iris_versicolor_3.jpg)

Three Choices for Number of Cluster Centers
========================================================

Different choices of desired number of cluster centers will group the underlying observations differently:

```{r, echo=FALSE}
set.seed(10)
par(mfrow = c(1,3))
results2 <- kmeans(iris[,-5],2)
plot(iris[c("Petal.Length", "Petal.Width")], col = results2$cluster,
     main =  "k=2")
results3 <- kmeans(iris[,-5],3)
plot(iris[c("Petal.Length", "Petal.Width")], col = results3$cluster,
     main =  "k=3")
results5 <- kmeans(iris[,-5],5)
plot(iris[c("Petal.Length", "Petal.Width")], col = results5$cluster,
     main = "k=5")
```

K-means Demonstration Application
=======================================================
left: 50%
Our **shiny** [application](http://majystr.shinyapps.io/Project) allows the observer to see the effect of different number of clusters on groupings of the iris species.  The following tables show the confusion matrices for three such settings: 2, 3, 5.

```{r, echo=FALSE,results="asis"}
print(xtable(table(iris$Species,results2$cluster)),type='html',comments=FALSE)
print(xtable(table(iris$Species,results3$cluster)),type='html',comments=FALSE)
```

***

```{r, echo=FALSE,results="asis"}
print(xtable(table(iris$Species,results5$cluster)),type='html',comments=FALSE)
```

As you can see, the number of *k* is an important parameter in determine accuracy of grouping.  There are advanced techniques to optimally determine these cluster numbers, but they are beyond the scope of this assignment.

In the meantime, please feel free to use this application in all your k-means demonstrations.